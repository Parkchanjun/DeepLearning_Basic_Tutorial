{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_Basic_Tutorial_Chanjun_Park.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8_IQwXBrC3Y",
        "colab_type": "text"
      },
      "source": [
        "제작자: Chanjun Park (박찬준)<br>\n",
        "소속: Korea University NLP&AI Lab (고려대학교 자연어처리&인공지능 연구실)<br>\n",
        "\n",
        "튜토리얼 명: Keras로 배우는 Artificial Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiHva9JjqyDI",
        "colab_type": "text"
      },
      "source": [
        "<1>간단한 인공신경망 실습을 진행해봅시다.\n",
        "숫자 5개 중 2개를 학습해 나머지 3개를 예측해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cNarp4Fl7l2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy\n",
        "\n",
        "x=numpy.array([0,1,2,3,4]) #숫자 5개 선언\n",
        "y=x*2+1 #y값 정의\n",
        "\n",
        "model=keras.models.Sequential() #인스턴스 만들기\n",
        "model.add(keras.layers.Dense(1,input_shape=(1,)))#계층 추가/입력은 1개\n",
        "\n",
        "model.compile('SGD','mse') #어떻게 학습할지 적기 (최적화알고리즘, 손실함수)\n",
        "\n",
        "model.fit(x[:2],y[:2],epochs=1000,verbose=0) #실제 학습하기 , verbose가 0일경우 학습진행 상황 표시 X, 2개는 학습 진행\n",
        "\n",
        "print('Targets: ',y[2:]) #3개는 예측을 진행\n",
        "print('Predictions:',model.predict(x[2:]).flatten()) #실제 예측하기\n",
        "print('Predictions:',model.predict(x[2:])) #flatten을 빼면 각각 표시됨."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RybMNwYLrHBf",
        "colab_type": "text"
      },
      "source": [
        "ANN (인공신경망)\n",
        "얕은 신경망 shallow Neural Network\n",
        "\n",
        "핵심은 입력,은닉,출력 계층으로 이루어져 있다 !\n",
        "\n",
        "1단계: 입력이 들어오면 가중치 행렬을 곱해준다. <br>\n",
        "2단계: 이를 은닉 행렬의 입력으로 넣어준다.<br>\n",
        "3단계: 은닉 계층에서는 입력된 값에 활성화 함수를 적용한다. 이는 비선형성이라는 특징을 부여한다. <br>\n",
        "4단계: 여기에 또 다시 가중치 행렬을 곱하고 출력계층으로 보내준다.<br>\n",
        "5단계: 출력계층은 마찬가지로 활성화 함수를 적용하고 최종적으로 출력값을 내보낸다.<br>\n",
        "6단계: 만약 분류 문제일 경우 소프트맥스 연산을 사용한다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fORI8jUisacX",
        "colab_type": "text"
      },
      "source": [
        "분류와 회귀<br>\n",
        "분류: 0~9 숫자 중 무엇이냐, 스팸 햄 메일 중 어떤거?<br>\n",
        "회귀: 일주일간 온도가 이랬는데 내일 온도는? 연속성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVGZ9Rphw4gA",
        "colab_type": "text"
      },
      "source": [
        "Mnist 예제로 학습해보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzZxm-0a2mYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras import datasets\n",
        "from keras.utils import np_utils\n",
        "from keras import layers,models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "#이곳은 하이퍼파라미터를 설정하는 곳 입니다.\n",
        "HPARAMS = defaultdict(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics='accuracy',\n",
        "    n_epoch=10,\n",
        "    batch_size=100,\n",
        "    validation_split=0.2 \n",
        ")\n",
        "\n",
        "hparams = type('', (object,), HPARAMS)() # dict to class\n",
        "\n",
        "class ANN_MNIST(models.Sequential):\n",
        "  def __init__(self,Nin,Nh,Nout):\n",
        "    super().__init__()\n",
        "    self.add(layers.Dense(Nh,activation='relu',input_shape=(Nin,)))\n",
        "    self.add(layers.Dense(Nout,activation='softmax'))\n",
        "    self.compile(loss=hparams.loss,optimizer=hparams.optimizer,metrics=[hparams.metrics])\n",
        "    \n",
        "def one_hot(X_test,Y_test): \n",
        "  X_test=np_utils.to_categorical(X_test)\n",
        "  Y_test=np_utils.to_categorical(Y_test)\n",
        "  \n",
        "  return X_test,Y_test\n",
        "\n",
        "def make_2d(X_train,X_test):\n",
        "  L,H,W=X_train.shape\n",
        "  X_train=X_train.reshape(-1, H*W)\n",
        "  X_test=X_test.reshape(-1, H*W)\n",
        "  \n",
        "  X_train=X_train/255.0\n",
        "  X_test=X_test/255.0\n",
        "  \n",
        "  \n",
        "  return X_train,X_test\n",
        "\n",
        "def data_load():\n",
        "  (X_train,Y_train),(X_test,Y_test)=datasets.mnist.load_data()\n",
        "  \n",
        "  Y_train,Y_test=one_hot(Y_train,Y_test)\n",
        "  \n",
        "  X_train,X_test=make_2d(X_train,X_test)\n",
        "  \n",
        "  return (X_train, X_test, Y_train, Y_test)\n",
        "\n",
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('Model Loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train','Test'],loc=0) #두선의 이름 표시\n",
        "\n",
        "def plot_acc(history):\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('Model Accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train','Test'],loc=0)\n",
        "  \n",
        "def main():\n",
        "  Nin=784\n",
        "  Nh=100\n",
        "  Nout=10\n",
        "  \n",
        "  (X_train,X_test,Y_train,Y_test)=data_load()\n",
        "  \n",
        "  \n",
        "  model=ANN_MNIST(Nin,Nh,Nout)\n",
        "  \n",
        "  history= model.fit(X_train,Y_train,epochs=hparams.n_epoch,batch_size=hparams.batch_size,validation_split=hparams.validation_split)\n",
        "  performance_test=model.evaluate(X_test,Y_test,batch_size=hparams.batch_size)\n",
        "  print('Test Loss and Accuracy -> ',performance_test)\n",
        "  \n",
        "  plot_loss(history)\n",
        "  plt.show()\n",
        "  \n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dazk67NjLjsi",
        "colab_type": "text"
      },
      "source": [
        "회귀 문제를 이제 풀어보자\n",
        "보스턴 집값 문제\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtxycgkmLu3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers,models\n",
        "from keras import datasets\n",
        "from sklearn import preprocessing\n",
        "\n",
        "class ANN(models.Model):\n",
        "  def __init__(self,Nin,Nh,Nout):\n",
        "    hidden=layers.Dense(Nh)\n",
        "    output=layers.Dense(Nout)\n",
        "    relu=layers.Activation('relu')\n",
        "    \n",
        "    \n",
        "    #연결 정의\n",
        "    x=layers.Input(shape=(Nin,))\n",
        "    h=relu(hidden(x))\n",
        "    y=output(h)\n",
        "    \n",
        "    super().__init__(x,y)\n",
        "    \n",
        "    self.compile(loss='mse',optimizer='sgd')\n",
        "\n",
        "def data_load():\n",
        "  (X_train,y_train),(X_test,y_test)=datasets.boston_housing.load_data()\n",
        "  scaler=preprocessing.MinMaxScaler()\n",
        "  X_train=scaler.fit_transform(X_train)\n",
        "  X_test=scaler.fit_transform(X_test)\n",
        "  \n",
        "  return X_train,y_train,X_test,y_test\n",
        "\n",
        "def main():\n",
        "  Nin=13\n",
        "  Nh=5\n",
        "  Nout=1\n",
        "  \n",
        "  X_train,y_train,X_test,y_test=data_load()\n",
        "  \n",
        "  model=ANN(Nin,Nh,Nout)\n",
        "  history=model.fit(X_train,y_train,epochs=100,batch_size=100,validation_split=0.2,verbose=2)\n",
        "  \n",
        "  \n",
        "  performance_test=model.evaluate(X_test,y_test,batch_size=100)\n",
        "  print('Test Loss -> {:.2f}'.format(performance_test))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OewLNY_SP9j3",
        "colab_type": "text"
      },
      "source": [
        "ANN을 지나 이제 DNN을 배워보자\n",
        "ANN은 은닉의 출력이 출력의 입력으로\n",
        "DNN은 은닉의 출력이 새로운 은닉의 입력으로\n",
        "단지 그 차이일 뿐......."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4moCWPIQAze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers,models\n",
        "from keras import datasets\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "#이곳은 하이퍼파라미터를 설정하는 곳 입니다.\n",
        "HPARAMS = defaultdict(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics='accuracy',\n",
        "    n_epoch=10,\n",
        "    batch_size=100,\n",
        "    validation_split=0.2 \n",
        ")\n",
        "\n",
        "hparams = type('', (object,), HPARAMS)() # dict to class\n",
        "\n",
        "\n",
        "class DNN(models.Sequential):\n",
        "  def __init__(self,Nin,Nh_1,Nout):\n",
        "    super().__init__()\n",
        "    \n",
        "    #입력 계층의 정의는 첫번째 hidden을 정의할 때 같이 이루어지게 된다.\n",
        "    self.add(layers.Dense(Nh_1[0],activation='relu',input_shape=(Nin,),name='Hidden-1'))\n",
        "    self.add(layers.Dropout(0.2))\n",
        "    \n",
        "    #제2계층부터는 케라스가 자동으로 현재 계층의 입력 노드 수를 앞에 나온 은닉계층의 출력 수로 설정해준다.\n",
        "    self.add(layers.Dense(Nh_1[1],activation='relu',name='Hidden-2'))\n",
        "    self.add(layers.Dropout(0.2))\n",
        "    self.add(layers.Dense(Nout,activation='softmax'))\n",
        "    \n",
        "    self.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "\n",
        "def one_hot(X_test,Y_test): \n",
        "  X_test=np_utils.to_categorical(X_test)\n",
        "  Y_test=np_utils.to_categorical(Y_test)\n",
        "  \n",
        "  return X_test,Y_test\n",
        "\n",
        "def make_2d(X_train,X_test):\n",
        "  L,H,W=X_train.shape\n",
        "  X_train=X_train.reshape(-1, H*W)\n",
        "  X_test=X_test.reshape(-1, H*W)\n",
        "  \n",
        "  X_train=X_train/255.0\n",
        "  X_test=X_test/255.0\n",
        "  \n",
        "  \n",
        "  return X_train,X_test\n",
        "\n",
        "def data_load():\n",
        "  (X_train,Y_train),(X_test,Y_test)=datasets.mnist.load_data()\n",
        "  \n",
        "  Y_train,Y_test=one_hot(Y_train,Y_test)\n",
        "  \n",
        "  X_train,X_test=make_2d(X_train,X_test)\n",
        "  \n",
        "  return (X_train, X_test, Y_train, Y_test)   \n",
        "    \n",
        "def main():\n",
        "  #기본 파라미터 설정\n",
        "  Nin=784\n",
        "  Nh_1=[100,50] #2개의 은닉층\n",
        "  Nout=10\n",
        "  \n",
        "  (X_train,X_test,Y_train,Y_test)=data_load()\n",
        "  \n",
        "  model=DNN(Nin,Nh_1,Nout)\n",
        "  \n",
        "  history= model.fit(X_train,Y_train,epochs=hparams.n_epoch,batch_size=hparams.batch_size,validation_split=hparams.validation_split)\n",
        "  performance_test=model.evaluate(X_test,Y_test,batch_size=hparams.batch_size)\n",
        "  print('Test Loss and Accuracy -> ',performance_test)\n",
        "  \n",
        "  plot_loss(history)\n",
        "  plt.show()\n",
        "  \n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-SbpP49Tmk5",
        "colab_type": "text"
      },
      "source": [
        "이번에는 컬러 이미지를 분류해보자.\n",
        "RGB이니깐 32 * 32 * 3 이다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50qDycoyTpG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers,models\n",
        "from keras import datasets\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "  \n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "#이곳은 하이퍼파라미터를 설정하는 곳 입니다.\n",
        "HPARAMS = defaultdict(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics='accuracy',\n",
        "    n_epoch=10,\n",
        "    batch_size=100,\n",
        "    validation_split=0.2 \n",
        ")\n",
        "\n",
        "hparams = type('', (object,), HPARAMS)() # dict to class\n",
        "\n",
        "\n",
        "class DNN(models.Sequential):\n",
        "  def __init__(self,Nin,Nh_1,pd_l,Nout):\n",
        "    super().__init__()\n",
        "    \n",
        "    #입력 계층의 정의는 첫번째 hidden을 정의할 때 같이 이루어지게 된다.\n",
        "    self.add(layers.Dense(Nh_1[0],activation='relu',input_shape=(Nin,),name='Hidden-1'))\n",
        "    self.add(layers.Dropout(pd_l[0]))\n",
        "    \n",
        "    #제2계층부터는 케라스가 자동으로 현재 계층의 입력 노드 수를 앞에 나온 은닉계층의 출력 수로 설정해준다.\n",
        "    self.add(layers.Dense(Nh_1[1],activation='relu',name='Hidden-2'))\n",
        "    self.add(layers.Dropout(pd_l[1]))\n",
        "\n",
        "    self.add(layers.Dense(Nout,activation='softmax'))\n",
        "    \n",
        "    self.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "\n",
        "def data_load():\n",
        "  (X_train,y_train),(X_test,y_test)=datasets.cifar10.load_data()\n",
        "\n",
        "  Y_train=np_utils.to_categorical(y_train)\n",
        "  y_test=np_utils.to_categorical(y_test)\n",
        "  \n",
        "  L,W,H,C=X_train.shape #50000 32 32 3\n",
        "  #print(L,H,H,C)\n",
        "  X_train=X_train.reshape(-1,W*H*C)\n",
        "  X_test=X_test.reshape(-1,W*H*C)\n",
        "\n",
        "  return (X_train,X_test,Y_train,Y_test)\n",
        "\n",
        "def main():\n",
        "  #기본 파라미터 설정\n",
        "  Nh_1=[100,50] #2개의 은닉층\n",
        "  pd_l=[0.0,0.0]\n",
        "  Nout=10\n",
        "  \n",
        "  (X_train,X_test,Y_train,Y_test)=data_load()\n",
        "  \n",
        "  model=DNN(X_train.shape[1],Nh_1,pd_l,Nout) #입력, 은닉개수,드롭아웃 비율, 출력\n",
        "  \n",
        "  history= model.fit(X_train,Y_train,epochs=hparams.n_epoch,batch_size=hparams.batch_size,validation_split=hparams.validation_split)\n",
        "  performance_test=model.evaluate(X_test,Y_test,batch_size=hparams.batch_size)\n",
        "  print('Test Loss and Accuracy -> ',performance_test)\n",
        " \n",
        "  \n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXrZtbBzXHja",
        "colab_type": "text"
      },
      "source": [
        "이제는 CNN입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYA0hNXsa6uh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers,models\n",
        "from keras import datasets\n",
        "from keras.utils import np_utils\n",
        "from keras import backend\n",
        "import keras\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class CNN(models.Sequential):\n",
        "  def __init__(self,input_shape,num_classes):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.add(layers.Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=input_shape)) #커널개수, 커널사이즈, 활성화함수, 입력 \n",
        "    self.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
        "    self.add(layers.MaxPooling2D(pool_size=(2,2))) #2x2 셀로 묶어서 가장 큰값을 내보낸다.\n",
        "    self.add(layers.Dropout(0.25))\n",
        "    \n",
        "    self.add(layers.Flatten())#완전연결 계층으로 들어간다.\n",
        "   \n",
        "    self.add(layers.Dense(128,activation='relu')) #은닉 계층은 길이가 128로\n",
        "    self.add(layers.Dropout(0.5))\n",
        "    self.add(layers.Dense(num_classes,activation='softmax'))\n",
        "    \n",
        "    self.compile(loss=keras.losses.categorical_crossentropy,optimizer='rmsprop',metrics=['accuracy'])\n",
        "    \n",
        "\n",
        "class DATA():\n",
        "  def __init__(self):\n",
        "    num_classes=10\n",
        "    (x_train,y_train),(x_test,y_test)=datasets.mnist.load_data()\n",
        "    print(x_train.shape) #60000,784 (원래)\n",
        "    \n",
        "    #img_rows,img_cols=X_train.shape[1:]\n",
        "    img_rows,img_cols=int(28),int(28) \n",
        "    print(img_rows,img_cols)\n",
        "    \n",
        "    if backend.image_data_format()=='channels_first':\n",
        "      x_train=x_train.reshape(x_train.shape[0],1,img_rows,img_cols)\n",
        "      x_test=x_test.reshape(x_test.shape[0],1,img_rows,img_cols)\n",
        "      input_shape=(1,img_rows,img_cols) #컬러 채널을 임의로 첫번째로 해준다.\n",
        "    else:\n",
        "      x_train=x_train.reshape(x_train.shape[0],img_rows,img_cols,1)\n",
        "      x_test=x_test.reshape(x_test.shape[0],img_rows,img_cols,1)\n",
        "      input_shape=(img_rows,img_cols,1) #마지막에 컬러 채널을 임의로 첫번째로 해준다.\n",
        "     \n",
        "    \n",
        "    x_train=x_train.astype('float32')\n",
        "    x_test=x_test.astype('float32')\n",
        "    x_train/=255\n",
        "    x_test/=255\n",
        "    \n",
        "    y_train=keras.utils.to_categorical(y_train,num_classes)\n",
        "    y_test=keras.utils.to_categorical(y_test,num_classes)\n",
        "    \n",
        "    \n",
        "    self.input_shape=input_shape\n",
        "    self.num_classes=num_classes\n",
        "    \n",
        "    self.x_train,self.y_train=x_train,y_train\n",
        "    self.x_test,self.y_test=x_test,y_test\n",
        "    \n",
        "       \n",
        "\n",
        "def main():\n",
        "  batch_size=128\n",
        "  epochs=10\n",
        "  \n",
        "  data=DATA()\n",
        "  model=CNN(data.input_shape,data.num_classes)\n",
        "  \n",
        "  history= model.fit(data.x_train,data.y_train,batch_size=batch_size,epochs=epochs,validation_split=0.2)\n",
        "  \n",
        "  score=model.evaluate(data.x_test,data.y_test)\n",
        "  print()\n",
        "  print('Test Loss',score[0])\n",
        "  print('Test Accuracy',score[1])\n",
        "  \n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}